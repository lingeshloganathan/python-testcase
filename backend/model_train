"""
Modern Gymnasium version of the PPO agent for predicting user story priority
based on test result timelines.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from sklearn.preprocessing import MinMaxScaler

# ===============================
# Load your data
# ===============================
userstory = pd.read_csv(r"D:\project-testcase\backend\userstory_commit_report.csv")
tests = pd.read_csv(r"D:\project-testcase\tests\results\test_results.csv", parse_dates=["Timestamp"])
tests["Status"] = tests["Status"].str.upper().fillna("PASSED")

# ===============================
# Build timeline per UserStory
# ===============================
def build_timeline(userstory_df, tests_df):
    # Since you have one story now, just sort tests by time
    return tests_df.sort_values("Timestamp").reset_index(drop=True)

timeline = build_timeline(userstory, tests)


# ===============================
# Define the Gymnasium environment
# ===============================
class PriorityEnv(gym.Env):
    """
    Gymnasium environment to learn to assign priorities (Low, Medium, High)
    based on evolving test results.
    """

    metadata = {"render_modes": ["human"], "render_fps": 2}

    def __init__(self, timeline, meta=None, fix_horizon=2):
        super().__init__()
        self.timeline = timeline
        self.meta = meta or {}
        self.ptr = 0
        self.alpha = 0.7  # weight for failure_age_days
        self.beta = 0.3   # weight for failure_rate
        self.penalty = 0.02
        self.fix_bonus = 1.0
        self.fix_horizon = fix_horizon

        # Observation space: [failure_rate, failure_age_days, total_tests, failed_tests]
        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(4,), dtype=np.float32)
        # Action space: 0=Low, 1=Medium, 2=High
        self.action_space = spaces.Discrete(3)
        self.scaler = MinMaxScaler()

    def _get_obs(self):
        window = self.timeline.iloc[: self.ptr + 1]
        total = len(window)
        failed = (window["Status"] == "FAILED").sum()
        failure_rate = failed / total
        failure_age = 0.0
        if failed > 0:
            first_fail = window[window["Status"] == "FAILED"]["Timestamp"].min()
            failure_age = (self.now - first_fail).total_seconds() / (3600 * 24)

        vec = np.array([failure_rate, failure_age, total, failed], dtype=np.float32).reshape(1, -1)
        # Normalize dynamically
        self.scaler.fit(np.vstack([vec, np.array([[1, 1, 50, 50]])]))
        obs = self.scaler.transform(vec)[0]
        return obs.astype(np.float32)

    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)
        self.ptr = 0
        self.now = self.timeline.iloc[0]["Timestamp"]
        obs = self._get_obs()
        info = {}
        return obs, info

    def step(self, action):
        assert self.action_space.contains(action)
        window = self.timeline.iloc[: self.ptr + 1]
        total = len(window)
        failed = (window["Status"] == "FAILED").sum()
        failure_rate = failed / total
        failure_age = 0.0
        if failed > 0:
            first_fail = window[window["Status"] == "FAILED"]["Timestamp"].min()
            failure_age = (self.now - first_fail).total_seconds() / (3600 * 24)

        # Reward shaping
        priority_value = action / 2.0
        reward = priority_value * (self.alpha * failure_age + self.beta * failure_rate) - self.penalty * action

        # Delayed fix bonus
        if action == 2 and failed > 0:
            future_end = min(self.ptr + self.fix_horizon, len(self.timeline) - 1)
            future = self.timeline.iloc[self.ptr + 1 : future_end + 1]
            past_failed = (window["Status"] == "FAILED").any()
            future_pass = (future["Status"] == "PASSED").any()
            if past_failed and future_pass:
                reward += self.fix_bonus

        # Advance pointer
        self.ptr += 1
        terminated = self.ptr >= len(self.timeline) - 1
        truncated = False
        if not terminated:
            self.now = self.timeline.iloc[self.ptr]["Timestamp"]

        obs = self._get_obs()
        info = {"step": self.ptr, "failure_rate": failure_rate, "failure_age": failure_age}
        return obs, float(reward), terminated, truncated, info

    def render(self):
        print(f"Step {self.ptr}/{len(self.timeline)}")

    def close(self):
        pass


# ===============================
# Train PPO
# ===============================
meta = {"num_commits": 1, "num_files": 1, "num_functions": 2, "num_authors": 1}
env = PriorityEnv(timeline, meta)
vec_env = DummyVecEnv([lambda: env])

model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    learning_rate=3e-4,
    gamma=0.99,
    tensorboard_log="./ppo_priority_gymnasium_logs",
)

model.learn(total_timesteps=25)
model.save("ppo_priority_gymnasium_model")

# ===============================
# Evaluate model
# ===============================
obs, info = env.reset()
done = False
total_reward = 0
while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(int(action))
    print(
        f"Step={info['step']:<3} | Action={['Low','Med','High'][int(action)]} | "
        f"FailRate={info['failure_rate']:.2f} | FailAge={info['failure_age']:.2f} | Reward={reward:.3f}"
    )
    total_reward += reward
    done = terminated or truncated

print("Total Episode Reward:", total_reward)
